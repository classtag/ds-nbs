{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# Toxic ngram tfidf and multi-models stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T14:36:30.742444Z",
     "start_time": "2018-12-12T14:36:29.941828Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# feature extraction and model selection tools.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn import decomposition, preprocessing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import scipy.sparse as scs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T14:36:30.749648Z",
     "start_time": "2018-12-12T14:36:30.745213Z"
    }
   },
   "outputs": [],
   "source": [
    "## basic paramaters definition\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T14:36:32.449431Z",
     "start_time": "2018-12-12T14:36:30.753748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_trn (159571, 8)\n",
      "df_tst (153164, 2)\n"
     ]
    }
   ],
   "source": [
    "df_trn = pd.read_csv('../input/train.csv').fillna(' ')\n",
    "df_tst = pd.read_csv('../input/test.csv').fillna(' ')\n",
    "\n",
    "print('df_trn', df_trn.shape)\n",
    "print('df_tst', df_tst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop at local should sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.808Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "is_dev = False\n",
    "if is_dev:\n",
    "    samples = []\n",
    "    for label in class_names:\n",
    "        df_sample = df_trn.groupby(\n",
    "            label,\n",
    "            group_keys=False).apply(lambda x: x.sample(min(len(x), 10000)))\n",
    "        samples.append(df_sample)\n",
    "\n",
    "    df_trn = pd.concat(samples, axis=0)\n",
    "    df_trn = df_trn.drop_duplicates()\n",
    "    df_tst = df_tst.sample(10000)\n",
    "    print(\"Sample size is \", df_trn.shape, df_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump dataframe start 2018-12-12 14:36:32.467692\n",
      "dump dataframe end 2018-12-12 14:36:33.279240\n"
     ]
    }
   ],
   "source": [
    "print('dump dataframe start', dt.now())\n",
    "pickle.dump(df_trn, open('df_trn.pkl', 'wb'))\n",
    "pickle.dump(df_tst, open('df_tst.pkl', 'wb'))\n",
    "print('dump dataframe end', dt.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF feature extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF start 2018-12-12 14:36:33.304008\n",
      "TF-IDF 1/3 2018-12-12 14:41:15.827396\n",
      "TF-IDF 2/3 2018-12-12 14:44:16.510873\n",
      "TF-IDF 3/3 2018-12-12 14:46:54.036578\n",
      "TF-IDF end 2018-12-12 14:46:54.037655 (159571, 60000) (153164, 60000)\n",
      "save_npz start 2018-12-12 14:46:54.037911\n",
      "save_npz end 2018-12-12 14:52:53.095288\n"
     ]
    }
   ],
   "source": [
    "# extract word level ngram and char level ngram feature\n",
    "trn_text = df_trn['comment_text']\n",
    "tst_text = df_tst['comment_text']\n",
    "all_text = pd.concat([trn_text, tst_text])\n",
    "\n",
    "# 提取单词级别的tfidf文本特征，ngram设为(1,2) 表示1到2个单词的分割都会提取出来\n",
    "word_vec = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "\n",
    "# 提取字符级别的tfidf特征，ngram设为(2,6) 这个地方，可以调参，不过50K的特征，其实覆盖度也够了\n",
    "char_vec = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=50000)\n",
    "\n",
    "# 把两个向量化特征提取工具做成一个pipeline\n",
    "all_vec = make_union(word_vec, char_vec, n_jobs=10)\n",
    "\n",
    "# 训练的时候需要用全量的数据来让tfidf覆盖到你所有的单词，词组，字符，字符串，能与计算出这部分的tfidf\n",
    "print('TF-IDF start', dt.now())\n",
    "all_vec.fit(all_text)\n",
    "pickle.dump(all_vec, open('tfidf.vec', 'wb'))\n",
    "\n",
    "print('TF-IDF 1/3', dt.now())\n",
    "X_trn = all_vec.transform(trn_text)\n",
    "print('TF-IDF 2/3', dt.now())\n",
    "X_tst = all_vec.transform(tst_text)\n",
    "print('TF-IDF 3/3', dt.now())\n",
    "print('TF-IDF end', dt.now(), X_trn.shape, X_tst.shape)\n",
    "\n",
    "print('save_npz start', dt.now())\n",
    "scs.save_npz('X_trn_tfidf.npz', X_trn)\n",
    "scs.save_npz('X_tst_tfidf.npz', X_tst)\n",
    "print('save_npz end', dt.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing features dimensions with PCA and StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.819Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TruncatedSVD start 2018-12-12 14:52:53.111774\n",
      "TruncatedSVD 1/3 2018-12-12 14:56:29.596487\n",
      "TruncatedSVD 2/3 2018-12-12 14:56:45.288917\n",
      "TruncatedSVD 3/3 2018-12-12 14:56:59.203840\n",
      "TruncatedSVD end 2018-12-12 14:56:59.204074 (159571, 120) (153164, 120)\n",
      "StandardScaler start 2018-12-12 14:56:59.407111\n",
      "StandardScaler 1/3 2018-12-12 14:56:59.999576\n",
      "StandardScaler 2/3 2018-12-12 14:57:00.210949\n",
      "StandardScaler 3/3 2018-12-12 14:57:00.414812\n",
      "StandardScaler end 2018-12-12 14:57:00.415168 (159571, 120) (153164, 120)\n"
     ]
    }
   ],
   "source": [
    "# 使用SVD进行降维，components设为120，SVD的components的合适调整区间一般为120~200\n",
    "# 因为tfidf提取的特征唯独太大，线性模型还可以快速训练处一个model，但是对于treebase的模型就差很多了，特征维度多了，简直就是灾难\n",
    "# 面对特征灾难的解法有：1.降维、2. 使用model-base的特征选择\n",
    "\n",
    "# 使用TruncatedSVD来降维，是因为它计算非常快，不像NMF这种计算量就非常大，耗时比较长\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "print('TruncatedSVD start', dt.now())\n",
    "svd.fit(X_trn)\n",
    "print('TruncatedSVD 1/3', dt.now())\n",
    "X_trn_svd = svd.transform(X_trn)\n",
    "print('TruncatedSVD 2/3', dt.now())\n",
    "X_tst_svd = svd.transform(X_tst)\n",
    "print('TruncatedSVD 3/3', dt.now())\n",
    "print('TruncatedSVD end', dt.now(), X_trn_svd.shape, X_tst_svd.shape)\n",
    "\n",
    "np.save('X_trn_svd.npy', X_trn_svd)\n",
    "np.save('X_tst_svd.npy', X_tst_svd)\n",
    "\n",
    "# 对从SVD获得的数据进行标准化处理，这样在线性模型或者神经网络这类模型中对特征范围比较敏感的非常有用\n",
    "scl = preprocessing.StandardScaler()\n",
    "print('StandardScaler start', dt.now())\n",
    "scl.fit(X_trn_svd)\n",
    "print('StandardScaler 1/3', dt.now())\n",
    "X_trn_svd_scl = scl.transform(X_trn_svd)\n",
    "print('StandardScaler 2/3', dt.now())\n",
    "X_tst_svd_scl = scl.transform(X_tst_svd)\n",
    "print('StandardScaler 3/3', dt.now())\n",
    "print('StandardScaler end', dt.now(), X_trn_svd_scl.shape, X_tst_svd_scl.shape)\n",
    "\n",
    "np.save('X_trn_svd_scl.npy', X_trn_svd_scl)\n",
    "np.save('X_tst_svd_scl.npy', X_tst_svd_scl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cache start 2018-12-12 14:57:00.626193\n",
      "Load cache end 2018-12-12 14:57:40.354163\n"
     ]
    }
   ],
   "source": [
    "print('Load cache start', dt.now())\n",
    "\n",
    "df_trn = pickle.load(open('df_trn.pkl', 'rb'))\n",
    "df_tst = pickle.load(open('df_tst.pkl', 'rb'))\n",
    "\n",
    "X_trn = scs.load_npz('X_trn_tfidf.npz')\n",
    "X_tst = scs.load_npz('X_tst_tfidf.npz')\n",
    "\n",
    "X_trn_svd = np.load('X_trn_svd.npy')\n",
    "X_tst_svd = np.load('X_tst_svd.npy')\n",
    "\n",
    "X_trn_svd_scl = np.load('X_trn_svd_scl.npy')\n",
    "X_tst_svd_scl = np.load('X_tst_svd_scl.npy')\n",
    "\n",
    "print('Load cache end', dt.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a general function to try different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.825Z"
    }
   },
   "outputs": [],
   "source": [
    "def skl_train_and_preds(clf, name, X_trn, X_tst):\n",
    "    print(\"Fit {} {}\".format(name, type(clf)))\n",
    "    print('X_trn', X_trn.shape)\n",
    "    print('X_tst', X_tst.shape)\n",
    "    scores = []  # 记录每一个类别的交叉验证训练的auc得分\n",
    "\n",
    "    trn_preds = pd.DataFrame.from_dict({'id': df_trn['id']})\n",
    "    tst_preds = pd.DataFrame.from_dict({'id': df_tst['id']})\n",
    "\n",
    "    # 这里迭代每一个类别来，将一个多label问题，转换为多个二分类问题\n",
    "    for class_name in class_names:\n",
    "        y_trn = df_trn[class_name]\n",
    "        # use cross-validataion to train model\n",
    "        cv_score = np.mean(\n",
    "            cross_val_score(clf, X_trn, y_trn, cv=3, scoring='roc_auc'))\n",
    "        scores.append(cv_score)\n",
    "        print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "        # fit new model with full of train data\n",
    "        clf.fit(X_trn, y_trn)\n",
    "        # used for stacking\n",
    "        trn_preds[class_name] = clf.predict_proba(X_trn)[:, 1]\n",
    "        # prediction the test for single submission or bledding\n",
    "        tst_preds[class_name] = clf.predict_proba(X_tst)[:, 1]\n",
    "\n",
    "    print('Total CV score is {}'.format(np.mean(scores)))\n",
    "    # save the model train and test predicted result in to local file.\n",
    "    trn_preds.to_csv('0-trn-ngram-{}.csv'.format(name), index=False)\n",
    "    tst_preds.to_csv('0-tst-ngram-{}.csv'.format(name), index=False)\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit lr <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "X_trn (159571, 60000)\n",
      "X_tst (153164, 60000)\n",
      "CV score for class toxic is 0.9785644059424624\n",
      "CV score for class severe_toxic is 0.9886992060449481\n",
      "CV score for class obscene is 0.9902203632571126\n",
      "CV score for class threat is 0.9891529445940939\n",
      "CV score for class insult is 0.9825933972596332\n",
      "CV score for class identity_hate is 0.9828002905348693\n",
      "Total CV score is 0.98533843460552\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 先参数最简单的逻辑回归模型，如果LR可以解决这个问题，说明我们的建模是没有问题的\n",
    "# 参数solver，使用sag计算非常快，其他solver你还可以选择比较牛逼的libnear，但是计算量稍大\n",
    "clf = LogisticRegression(solver='sag')\n",
    "skl_train_and_preds(clf, 'lr', X_trn, X_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T12:38:43.016994Z",
     "start_time": "2018-12-11T12:25:54.274Z"
    }
   },
   "source": [
    "## Feature selection before tree-base modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.829Z"
    },
    "code_folding": [
     3,
     4,
     7
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 定义一个通用的特征选择函数，用来为treebase的模型做特征降维\n",
    "def select_features(class_name, X_trn, X_tst, y_trn):\n",
    "    if os.path.exists('X_trn_sfm_{}.npz'.format(class_name)):\n",
    "        X_trn = scs.load_npz('X_trn_sfm_{}.npz'.format(class_name))\n",
    "        X_tst = scs.load_npz('X_tst_sfm_{}.npz'.format(class_name))\n",
    "    else:\n",
    "        # 使用LR做base model，这里就用默认参数吧，其实也能用pipeline的方式来做参数优化，比较耗时，没必要\n",
    "        lr = LogisticRegression(solver='sag')\n",
    "        # 这里的参数主要看threshold，这里是0.2是说我的特征权重大于0.2的才会留下来\n",
    "        sfm = SelectFromModel(lr, threshold=0.2)\n",
    "        print(X_trn.shape)\n",
    "        print('Feature selection 1/3')\n",
    "        sfm.fit(X_trn, y_trn)\n",
    "        print('Feature selection 2/3')\n",
    "        X_trn = sfm.transform(X_trn)\n",
    "        print('Feature selection 3/3')\n",
    "        X_tst = sfm.transform(X_tst)\n",
    "        print('feature selection end', type(X_trn), X_trn.shape, X_tst.shape)\n",
    "        scs.save_npz('X_trn_sfm_{}.npz'.format(class_name), X_trn)\n",
    "        scs.save_npz('X_tst_sfm_{}.npz'.format(class_name), X_tst)\n",
    "\n",
    "    return X_trn, X_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.831Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def xgb_train_and_preds(params,\n",
    "                        name,\n",
    "                        X_trn,\n",
    "                        X_tst,\n",
    "                        use_feature_selection=False):\n",
    "    print(\"Fit {} XGBClassifier\".format(name))\n",
    "    print('X_trn', X_trn.shape)\n",
    "    print('X_tst', X_tst.shape)\n",
    "\n",
    "    trn_preds = pd.DataFrame.from_dict({'id': df_trn['id']})\n",
    "    tst_preds = pd.DataFrame.from_dict({'id': df_tst['id']})\n",
    "\n",
    "    for class_name in class_names:\n",
    "        print('Train for class {}'.format(class_name))\n",
    "        y_trn = df_trn[class_name]\n",
    "        if use_feature_selection:\n",
    "            X_trn_sfm, X_tst_sfm = select_features(class_name, X_trn, X_tst,\n",
    "                                                   y_trn)\n",
    "        else:\n",
    "            X_trn_sfm, X_tst_sfm = X_trn, X_tst\n",
    "        trn_X, val_X, trn_y, val_y = train_test_split(\n",
    "            X_trn_sfm, y_trn, test_size=0.33, random_state=42, stratify=y_trn)\n",
    "\n",
    "        trn_ds, val_ds = xgb.DMatrix(trn_X, trn_y), xgb.DMatrix(val_X, val_y)\n",
    "        eval_list = [(val_ds, 'val')]\n",
    "\n",
    "        evals_result = {}\n",
    "        gbm = xgb.train(\n",
    "            params,\n",
    "            trn_ds,\n",
    "            3000,\n",
    "            eval_list,\n",
    "            early_stopping_rounds=30,\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=20,\n",
    "        )\n",
    "\n",
    "        best_iteration_ = np.argmax(evals_result['val']['auc']) + 1\n",
    "        print('best_iteration_', best_iteration_)\n",
    "        # fit new model with full of train data\n",
    "        gbm = xgb.train(params, xgb.DMatrix(X_trn_sfm, y_trn), best_iteration_)\n",
    "\n",
    "        # used for stacking\n",
    "        trn_preds[class_name] = gbm.predict(\n",
    "            xgb.DMatrix(X_trn_sfm), ntree_limit=gbm.best_ntree_limit)\n",
    "        # prediction the test for single submission or bledding\n",
    "        tst_preds[class_name] = gbm.predict(\n",
    "            xgb.DMatrix(X_tst_sfm), ntree_limit=gbm.best_ntree_limit)\n",
    "        print()\n",
    "\n",
    "    # save the model train and test predicted result in to local file.\n",
    "    trn_preds.to_csv('0-trn-ngram-{}.csv'.format(name), index=False)\n",
    "    tst_preds.to_csv('0-tst-ngram-{}.csv'.format(name), index=False)\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-12T14:36:29.834Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit xgb XGBClassifier\n",
      "X_trn (159571, 60000)\n",
      "X_tst (153164, 60000)\n",
      "Train for class toxic\n",
      "(159571, 60000)\n",
      "Feature selection 1/3\n",
      "Feature selection 2/3\n",
      "Feature selection 3/3\n",
      "feature selection end <class 'scipy.sparse.csr.csr_matrix'> (159571, 16169) (153164, 16169)\n",
      "[0]\tval-auc:0.821204\n",
      "Will train until val-auc hasn't improved in 30 rounds.\n",
      "[20]\tval-auc:0.942259\n",
      "[40]\tval-auc:0.95574\n",
      "[60]\tval-auc:0.960463\n",
      "[80]\tval-auc:0.963728\n",
      "[100]\tval-auc:0.96617\n",
      "[120]\tval-auc:0.968016\n",
      "[140]\tval-auc:0.969164\n",
      "[160]\tval-auc:0.969944\n",
      "[180]\tval-auc:0.97063\n",
      "[200]\tval-auc:0.971172\n",
      "[220]\tval-auc:0.971662\n",
      "[240]\tval-auc:0.971983\n",
      "[260]\tval-auc:0.972326\n",
      "[280]\tval-auc:0.972655\n",
      "[300]\tval-auc:0.9728\n",
      "[320]\tval-auc:0.973063\n",
      "[340]\tval-auc:0.973382\n",
      "[360]\tval-auc:0.973622\n",
      "[380]\tval-auc:0.973752\n",
      "[400]\tval-auc:0.973951\n",
      "[420]\tval-auc:0.974026\n",
      "[440]\tval-auc:0.974237\n",
      "[460]\tval-auc:0.974275\n",
      "[480]\tval-auc:0.974348\n",
      "[500]\tval-auc:0.974519\n",
      "[520]\tval-auc:0.974707\n",
      "[540]\tval-auc:0.974818\n",
      "[560]\tval-auc:0.974967\n",
      "[580]\tval-auc:0.975086\n",
      "[600]\tval-auc:0.975213\n",
      "[620]\tval-auc:0.975374\n",
      "[640]\tval-auc:0.975363\n",
      "[660]\tval-auc:0.975552\n",
      "[680]\tval-auc:0.975631\n",
      "[700]\tval-auc:0.975734\n",
      "[720]\tval-auc:0.975843\n",
      "[740]\tval-auc:0.975934\n",
      "[760]\tval-auc:0.975956\n",
      "[780]\tval-auc:0.97601\n",
      "[800]\tval-auc:0.976026\n",
      "[820]\tval-auc:0.976022\n",
      "Stopping. Best iteration:\n",
      "[802]\tval-auc:0.976043\n",
      "\n",
      "best_iteration_ 803\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': 13,\n",
    "    'eta': 1,\n",
    "    'silent': 1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'subsample': 0.88,\n",
    "    'colsample_bytree': 0.88,\n",
    "    'colsample_bylevel': 0.7,\n",
    "    'nthread': 10,\n",
    "    'learning_rate': 1e-1,\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "xgb_train_and_preds(params, 'xgb', X_trn, X_tst, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T05:47:44.456978Z",
     "start_time": "2018-12-13T05:47:44.438453Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def lgb_train_and_preds(params,\n",
    "                        name,\n",
    "                        X_trn,\n",
    "                        X_tst,\n",
    "                        use_feature_selection=False):\n",
    "    print(\"Fit {} LightGBM\".format(name))\n",
    "    print('X_trn', X_trn.shape)\n",
    "    print('X_tst', X_tst.shape)\n",
    "\n",
    "    trn_preds = pd.DataFrame.from_dict({'id': df_trn['id']})\n",
    "    tst_preds = pd.DataFrame.from_dict({'id': df_tst['id']})\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        print('Train for class {}'.format(class_name))\n",
    "        y_trn = df_trn[class_name]\n",
    "\n",
    "        if use_feature_selection:\n",
    "            X_trn_sfm, X_tst_sfm = select_features(class_name, X_trn, X_tst,\n",
    "                                                   y_trn)\n",
    "        else:\n",
    "            X_trn_sfm, X_tst_sfm = X_trn, X_tst\n",
    "\n",
    "        trn_X, val_X, trn_y, val_y = train_test_split(\n",
    "            X_trn_sfm, y_trn, test_size=0.33, random_state=42, stratify=y_trn)\n",
    "        trn_ds, val_ds = lgb.Dataset(trn_X, trn_y), lgb.Dataset(val_X, val_y)\n",
    "        evals_result = dict()\n",
    "        print('train with val...')\n",
    "        gbm = lgb.train(\n",
    "            params=params,\n",
    "            train_set=trn_ds,\n",
    "            valid_sets=val_ds,\n",
    "            valid_names=['val'],\n",
    "            early_stopping_rounds=30,\n",
    "            num_boost_round=3000,\n",
    "            verbose_eval=20,\n",
    "            evals_result=evals_result)\n",
    "\n",
    "        best_iteration_ = np.argmax(evals_result['val']['auc']) + 1\n",
    "        print('best_iteration_', best_iteration_)\n",
    "        # fit new model with full of train data\n",
    "        print('train for prediction...', dt.now())\n",
    "        gbm = lgb.train(\n",
    "            params=params,\n",
    "            train_set=lgb.Dataset(X_trn_sfm, y_trn),\n",
    "            num_boost_round=best_iteration_)\n",
    "        \n",
    "        # used for stacking\n",
    "        print('predict for train')\n",
    "        trn_preds[class_name] = gbm.predict(X_trn_sfm)\n",
    "        # prediction the test for single submission or bledding\n",
    "        print('predict for test')\n",
    "        tst_preds[class_name] = gbm.predict(X_tst_sfm)\n",
    "        print()\n",
    "\n",
    "    # save the model train and test predicted result in to local file.\n",
    "    trn_preds.to_csv('0-trn-ngram-{}.csv'.format(name), index=False)\n",
    "    tst_preds.to_csv('0-tst-ngram-{}.csv'.format(name), index=False)\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T06:14:31.559192Z",
     "start_time": "2018-12-13T05:47:45.578552Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit lgb LightGBM\n",
      "X_trn (159571, 60000)\n",
      "X_tst (153164, 60000)\n",
      "Train for class toxic\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.946243\n",
      "[40]\tval's auc: 0.958637\n",
      "[60]\tval's auc: 0.964901\n",
      "[80]\tval's auc: 0.968683\n",
      "[100]\tval's auc: 0.970932\n",
      "[120]\tval's auc: 0.972323\n",
      "[140]\tval's auc: 0.973072\n",
      "[160]\tval's auc: 0.97398\n",
      "[180]\tval's auc: 0.974428\n",
      "[200]\tval's auc: 0.97472\n",
      "[220]\tval's auc: 0.975139\n",
      "[240]\tval's auc: 0.975425\n",
      "[260]\tval's auc: 0.975628\n",
      "[280]\tval's auc: 0.975816\n",
      "[300]\tval's auc: 0.975967\n",
      "[320]\tval's auc: 0.976061\n",
      "[340]\tval's auc: 0.976164\n",
      "[360]\tval's auc: 0.976378\n",
      "[380]\tval's auc: 0.976353\n",
      "[400]\tval's auc: 0.976491\n",
      "[420]\tval's auc: 0.97656\n",
      "[440]\tval's auc: 0.976663\n",
      "[460]\tval's auc: 0.97676\n",
      "[480]\tval's auc: 0.976931\n",
      "[500]\tval's auc: 0.976939\n",
      "[520]\tval's auc: 0.977005\n",
      "[540]\tval's auc: 0.977117\n",
      "[560]\tval's auc: 0.977038\n",
      "Early stopping, best iteration is:\n",
      "[540]\tval's auc: 0.977117\n",
      "best_iteration_ 540\n",
      "train for prediction... 2018-12-13 05:52:55.838116\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class severe_toxic\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.981017\n",
      "[40]\tval's auc: 0.984669\n",
      "[60]\tval's auc: 0.985234\n",
      "[80]\tval's auc: 0.98539\n",
      "Early stopping, best iteration is:\n",
      "[66]\tval's auc: 0.985505\n",
      "best_iteration_ 66\n",
      "train for prediction... 2018-12-13 05:59:32.057489\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class obscene\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.986182\n",
      "[40]\tval's auc: 0.989436\n",
      "[60]\tval's auc: 0.990649\n",
      "[80]\tval's auc: 0.991416\n",
      "[100]\tval's auc: 0.991823\n",
      "[120]\tval's auc: 0.991947\n",
      "[140]\tval's auc: 0.99217\n",
      "[160]\tval's auc: 0.992235\n",
      "[180]\tval's auc: 0.992363\n",
      "[200]\tval's auc: 0.992452\n",
      "[220]\tval's auc: 0.992458\n",
      "[240]\tval's auc: 0.992507\n",
      "[260]\tval's auc: 0.992554\n",
      "[280]\tval's auc: 0.992545\n",
      "[300]\tval's auc: 0.992494\n",
      "Early stopping, best iteration is:\n",
      "[271]\tval's auc: 0.992566\n",
      "best_iteration_ 271\n",
      "train for prediction... 2018-12-13 06:02:13.444866\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class threat\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.691075\n",
      "Early stopping, best iteration is:\n",
      "[1]\tval's auc: 0.784249\n",
      "best_iteration_ 1\n",
      "train for prediction... 2018-12-13 06:04:50.565614\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class insult\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.968165\n",
      "[40]\tval's auc: 0.975819\n",
      "[60]\tval's auc: 0.978298\n",
      "[80]\tval's auc: 0.979242\n",
      "[100]\tval's auc: 0.979888\n",
      "[120]\tval's auc: 0.980185\n",
      "[140]\tval's auc: 0.980682\n",
      "[160]\tval's auc: 0.980873\n",
      "[180]\tval's auc: 0.981108\n",
      "[200]\tval's auc: 0.98119\n",
      "[220]\tval's auc: 0.981289\n",
      "[240]\tval's auc: 0.981254\n",
      "[260]\tval's auc: 0.981341\n",
      "[280]\tval's auc: 0.981321\n",
      "[300]\tval's auc: 0.981382\n",
      "[320]\tval's auc: 0.981548\n",
      "[340]\tval's auc: 0.981565\n",
      "[360]\tval's auc: 0.981501\n",
      "Early stopping, best iteration is:\n",
      "[346]\tval's auc: 0.981594\n",
      "best_iteration_ 346\n",
      "train for prediction... 2018-12-13 06:07:55.777312\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class identity_hate\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.96218\n",
      "[40]\tval's auc: 0.964588\n",
      "[60]\tval's auc: 0.967766\n",
      "[80]\tval's auc: 0.968807\n",
      "[100]\tval's auc: 0.96869\n",
      "Early stopping, best iteration is:\n",
      "[70]\tval's auc: 0.968861\n",
      "best_iteration_ 70\n",
      "train for prediction... 2018-12-13 06:11:56.869005\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "================================================================================\n",
      "Fit lgb-svd LightGBM\n",
      "X_trn (159571, 120)\n",
      "X_tst (153164, 120)\n",
      "Train for class toxic\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.901341\n",
      "[40]\tval's auc: 0.916545\n",
      "[60]\tval's auc: 0.924443\n",
      "[80]\tval's auc: 0.928844\n",
      "[100]\tval's auc: 0.931775\n",
      "[120]\tval's auc: 0.933692\n",
      "[140]\tval's auc: 0.935143\n",
      "[160]\tval's auc: 0.936382\n",
      "[180]\tval's auc: 0.937047\n",
      "[200]\tval's auc: 0.937414\n",
      "[220]\tval's auc: 0.937824\n",
      "[240]\tval's auc: 0.938285\n",
      "[260]\tval's auc: 0.93863\n",
      "[280]\tval's auc: 0.938702\n",
      "[300]\tval's auc: 0.938733\n",
      "[320]\tval's auc: 0.938818\n",
      "[340]\tval's auc: 0.938755\n",
      "[360]\tval's auc: 0.938975\n",
      "[380]\tval's auc: 0.93915\n",
      "[400]\tval's auc: 0.939211\n",
      "[420]\tval's auc: 0.939171\n",
      "[440]\tval's auc: 0.939202\n",
      "[460]\tval's auc: 0.93927\n",
      "[480]\tval's auc: 0.938299\n",
      "Early stopping, best iteration is:\n",
      "[462]\tval's auc: 0.939289\n",
      "best_iteration_ 462\n",
      "train for prediction... 2018-12-13 06:12:43.684614\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class severe_toxic\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.963655\n",
      "[40]\tval's auc: 0.970768\n",
      "[60]\tval's auc: 0.97499\n",
      "[80]\tval's auc: 0.976337\n",
      "[100]\tval's auc: 0.976929\n",
      "[120]\tval's auc: 0.977356\n",
      "[140]\tval's auc: 0.978049\n",
      "[160]\tval's auc: 0.978292\n",
      "[180]\tval's auc: 0.978633\n",
      "[200]\tval's auc: 0.979155\n",
      "[220]\tval's auc: 0.979498\n",
      "[240]\tval's auc: 0.979493\n",
      "[260]\tval's auc: 0.979662\n",
      "[280]\tval's auc: 0.97954\n",
      "Early stopping, best iteration is:\n",
      "[261]\tval's auc: 0.979684\n",
      "best_iteration_ 261\n",
      "train for prediction... 2018-12-13 06:13:05.608692\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class obscene\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.929729\n",
      "[40]\tval's auc: 0.945791\n",
      "[60]\tval's auc: 0.95198\n",
      "[80]\tval's auc: 0.955355\n",
      "[100]\tval's auc: 0.957574\n",
      "[120]\tval's auc: 0.959178\n",
      "[140]\tval's auc: 0.959876\n",
      "[160]\tval's auc: 0.960398\n",
      "[180]\tval's auc: 0.961017\n",
      "[200]\tval's auc: 0.961168\n",
      "[220]\tval's auc: 0.961271\n",
      "[240]\tval's auc: 0.96146\n",
      "[260]\tval's auc: 0.961637\n",
      "[280]\tval's auc: 0.961729\n",
      "[300]\tval's auc: 0.961706\n",
      "Early stopping, best iteration is:\n",
      "[279]\tval's auc: 0.961779\n",
      "best_iteration_ 279\n",
      "train for prediction... 2018-12-13 06:13:22.101893\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class threat\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.818237\n",
      "[40]\tval's auc: 0.848974\n",
      "[60]\tval's auc: 0.866645\n",
      "[80]\tval's auc: 0.877102\n",
      "[100]\tval's auc: 0.885058\n",
      "[120]\tval's auc: 0.895154\n",
      "[140]\tval's auc: 0.903573\n",
      "[160]\tval's auc: 0.910179\n",
      "[180]\tval's auc: 0.912618\n",
      "[200]\tval's auc: 0.915813\n",
      "[220]\tval's auc: 0.919853\n",
      "[240]\tval's auc: 0.9231\n",
      "[260]\tval's auc: 0.924618\n",
      "[280]\tval's auc: 0.928529\n",
      "[300]\tval's auc: 0.931725\n",
      "[320]\tval's auc: 0.933761\n",
      "[340]\tval's auc: 0.935183\n",
      "[360]\tval's auc: 0.935927\n",
      "[380]\tval's auc: 0.935679\n",
      "Early stopping, best iteration is:\n",
      "[365]\tval's auc: 0.936035\n",
      "best_iteration_ 365\n",
      "train for prediction... 2018-12-13 06:13:41.154018\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class insult\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.92814\n",
      "[40]\tval's auc: 0.938669\n",
      "[60]\tval's auc: 0.943835\n",
      "[80]\tval's auc: 0.946569\n",
      "[100]\tval's auc: 0.947738\n",
      "[120]\tval's auc: 0.948538\n",
      "[140]\tval's auc: 0.949124\n",
      "[160]\tval's auc: 0.949329\n",
      "[180]\tval's auc: 0.949593\n",
      "[200]\tval's auc: 0.949908\n",
      "[220]\tval's auc: 0.949982\n",
      "[240]\tval's auc: 0.950234\n",
      "[260]\tval's auc: 0.950379\n",
      "[280]\tval's auc: 0.95046\n",
      "[300]\tval's auc: 0.950493\n",
      "[320]\tval's auc: 0.950712\n",
      "[340]\tval's auc: 0.950859\n",
      "[360]\tval's auc: 0.950646\n",
      "[380]\tval's auc: 0.950841\n",
      "[400]\tval's auc: 0.950892\n",
      "Early stopping, best iteration is:\n",
      "[388]\tval's auc: 0.950989\n",
      "best_iteration_ 388\n",
      "train for prediction... 2018-12-13 06:14:02.086686\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "Train for class identity_hate\n",
      "train with val...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[20]\tval's auc: 0.900482\n",
      "[40]\tval's auc: 0.911286\n",
      "[60]\tval's auc: 0.916464\n",
      "[80]\tval's auc: 0.920542\n",
      "[100]\tval's auc: 0.919574\n",
      "[120]\tval's auc: 0.921368\n",
      "[140]\tval's auc: 0.92161\n",
      "[160]\tval's auc: 0.922514\n",
      "[180]\tval's auc: 0.9229\n",
      "[200]\tval's auc: 0.922855\n",
      "[220]\tval's auc: 0.922887\n",
      "Early stopping, best iteration is:\n",
      "[202]\tval's auc: 0.923493\n",
      "best_iteration_ 202\n",
      "train for prediction... 2018-12-13 06:14:20.031187\n",
      "predict for train\n",
      "predict for test\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 1e-1,\n",
    "    'metric': 'auc',\n",
    "    'max_depth': 13,\n",
    "    'objective': 'binary',\n",
    "    'subsample': 0.88,\n",
    "    'colsample_bytree': 0.88,\n",
    "    'random_state': 128\n",
    "}\n",
    "lgb_train_and_preds(params, 'lgb', X_trn, X_tst, True)\n",
    "lgb_train_and_preds(params, 'lgb-svd', X_trn_svd, X_tst_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T03:40:51.181Z"
    }
   },
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T06:16:17.176604Z",
     "start_time": "2018-12-13T06:16:17.172592Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_files = os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T06:16:22.028011Z",
     "start_time": "2018-12-13T06:16:18.243940Z"
    },
    "code_folding": [
     2,
     12
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-trn-ngram-lr.csv\n",
      "0-trn-ngram-lgb-svd.csv\n",
      "0-trn-ngram-stacking-sgd.csv\n",
      "0-trn-ngram-lgb.csv\n",
      "0-trn-ngram-xgb.csv\n",
      "0-tst-ngram-xgb.csv\n",
      "0-tst-ngram-lgb.csv\n",
      "0-tst-ngram-stacking-sgd.csv\n",
      "0-tst-ngram-lr.csv\n",
      "0-tst-ngram-lgb-svd.csv\n"
     ]
    }
   ],
   "source": [
    "trn_pred_dfs = []\n",
    "\n",
    "for trn_file in list(filter(lambda x: x.find('trn') >= 0 and x.find('csv')>=0, csv_files)):\n",
    "    print(trn_file)\n",
    "    column_names = [class_name + ' ' + trn_file for class_name in class_names]\n",
    "    df_pred = pd.read_csv(trn_file, index_col='id')\n",
    "    df_pred.columns = column_names\n",
    "    trn_pred_dfs.append(df_pred)\n",
    "\n",
    "trn_pred_df = pd.concat(trn_pred_dfs, axis=1)\n",
    "\n",
    "tst_pred_dfs = []\n",
    "for tst_file in list(filter(lambda x: x.find('tst') >= 0 and x.find('csv')>=0, csv_files)):\n",
    "    print(tst_file)\n",
    "    column_names = [class_name + ' ' + tst_file for class_name in class_names]\n",
    "    df_pred = pd.read_csv(tst_file, index_col='id')\n",
    "    df_pred.columns = column_names\n",
    "    tst_pred_dfs.append(df_pred)\n",
    "\n",
    "tst_pred_df = pd.concat(tst_pred_dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T06:16:23.779322Z",
     "start_time": "2018-12-13T06:16:23.774175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 30) (153164, 30)\n"
     ]
    }
   ],
   "source": [
    "print(trn_pred_df.shape, tst_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T06:16:38.250270Z",
     "start_time": "2018-12-13T06:16:24.894347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit stacking-sgd <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>\n",
      "X_trn (159571, 30)\n",
      "X_tst (153164, 30)\n",
      "CV score for class toxic is 0.9999913748814668\n",
      "CV score for class severe_toxic is 0.9999093414313679\n",
      "CV score for class obscene is 0.9999741251589508\n",
      "CV score for class threat is 0.9969840188951228\n",
      "CV score for class insult is 0.9999049058470703\n",
      "CV score for class identity_hate is 0.9997387363852877\n",
      "Total CV score is 0.9994170837665443\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(\n",
    "    loss='log',\n",
    "    penalty='l2',\n",
    "    alpha=1e-3,\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    tol=2e-5)\n",
    "skl_train_and_preds(clf, 'stacking-sgd', trn_pred_df.values, tst_pred_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
